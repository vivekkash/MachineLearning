{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-DL-Word Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM75MwK/gDFrx4ST4h0YPXe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekkash/MachineLearning/blob/master/NLP_DL_Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81CZxQhCvDkW",
        "colab_type": "text"
      },
      "source": [
        "# Brief Notes :\n",
        "\n",
        "## First thing in NLP is to convert the words into numbers or vectors as machine does'nt understand words this is called word embedding\n",
        "\n",
        "## word embedding \n",
        "## sentence embedding or doc embedding // will focus on this, as we need to process the entire document.\n",
        "\n",
        "## Embedding Tech\n",
        "## 1. Deterministic approach/ freq approach - no prob is being used -> count vectorization, just count the num of time words appears, \n",
        "##      Term freq TFID - no of words/ total words, this normalize way and better than count vector.\n",
        "## 2. Probability approach /stocashtic approach - utilization of context, meaning, prob.\n",
        "##.   - Word2vec -> has  CBOW continous bag of words model  and Skipgram\n",
        "##.   - GLOVE\n",
        "\n",
        "## The word embedding in CBOW is the output of the hidden layer of the NN once the model is train. (the context is passed as input & len or no of unique words is equal to len of i/p nodes of NN)\n",
        "\n",
        "## Skipgram we do opp of CBOW, we predict context words based on the target words. It prefered coz for one word word can have multiple context which suffice by skipgram\n",
        "\n",
        "## GLOVES takes cares of the subwords and work within the words, like unlike and like and its a pretrained model.\n",
        "\n",
        "## Gensim model which uses the gloves approach of word2vec\n",
        "https://radimrehurek.com/gensim/ \n",
        "\n",
        "## similarly,\n",
        "\n",
        "##spacy, textblob, sparknlp,  gensim\n",
        " https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
        "\n",
        "https://nlp.johnsnowlabs.com/docs/en/models\n",
        "\n",
        "### Latest EMBEDDING TECH - Bert, emlo, gpt2, googlenews (news article), fasttext (social media)."
      ]
    }
  ]
}